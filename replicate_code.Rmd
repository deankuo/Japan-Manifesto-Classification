---
title: "LDA modeling"
author: "PENG-TING, KUO"
date: "`r Sys.Date()`"
output: html_document
---

## Step 0: Load required packages

```{r Packages}
library(RMeCab) # to parse out words, stem words, and keep words; and to delete punctuation, numbers, and words appearing in less than 0.5% of the manifestos
require(quanteda)
require(quanteda.textmodels)
require(quanteda.textstats)
require(quanteda.textplots)
require(quanteda.corpora)
require(seededlda)
require(lubridate)
require(readtext) # to read in different types of text data
require(devtools)
require(newsmap) # classify documents based on "seed words" in dictionaries
require(seededlda) # run topic models
require(spacyr) # for part-of-speech tagging, entity recognition, and dependency parsing
options(width = 110)
```

## Step 1: Creating the corpus

```{r}
# set path
path_data <- '/Users/deankuo/Desktop/python/dessertation_replicate/dean_final_txt.csv'

# import csv file
dat_injp <- read.csv(path_data)
names(dat_injp)

# construct corpus
corp_injp <- corpus(dat_injp, text_field = "content")
summary(corp_injp, 5)

# rename the document names
docid <- paste(dat_injp$year, dat_injp$name_jp, dat_injp$kuname)
docnames(corp_injp) <- docid
print(corp_injp, 3)
```

## Step 2: Defining and delimiting documents

```{r}
# reshape document to the level of paragraphs
corp <- corpus_reshape(corp_injp, to = "paragraphs")
```

## Step 3: Defining and delimiting textual features

```{r}
# tokenize corpus and apply pre-processing
toks <- tokens(corp, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, padding = TRUE)     %>% 
    # remove grammatical words
    tokens_remove(pattern = stopwords("ja", source = "marimo"), padding = TRUE) %>%  
    tokens_select(pattern = "^[ぁ-んァ-ヶー一-龠]+$", valuetype = "regex", padding = TRUE)
print(toks[2], max_ndoc = 1, max_ntok = -1)
```

## Step 4: Further feature selection

```{r}
# read the predifined stop words by Amy
stopwords_amy <- readLines('/Users/deankuo/Desktop/python/dessertation_replicate/master_badterms_utf8.txt')

# perform collocation analysis
tstat_col <- toks %>% 
    tokens_select("^[ァ-ヶー一-龠]+$", valuetype = "regex", padding = TRUE) %>%  
    textstat_collocations()
head(tstat_col, 10)

# compound collocations
toks_comp <- tokens_compound(toks, tstat_col[tstat_col$z > 3,], concatenator = "") %>% 
  tokens_keep(min_nchar = 2)
print(toks_comp[2], max_ndoc = 1, max_ntok = -1)
```

```{r}
# construct document-feature matrix with compound collocations
dfmat_collo <- dfm(toks_comp) %>% 
    # remove tokens that appeared more than 99% and less than 0.5% of the manifestos
    dfm_trim(max_termfreg = 0.99, min_termfreq = 0.05, termfreq_type = "quantile",
             # TDMs were created for each election and words that appeared in less than 0.5% of manifestos for that election were removed.
             min_docfreq = 0.005, docfreq_type = "prop")

dfmat_collo_clean <- dfm_select(dfmat_collo, pattern = c(stopwords_amy), selection = "remove")
print(dfmat_collo_clean, 3)
```

## Step 5: Analysis of the documents and features

```{r}
# LDA with DFM with collocation
start_time <- Sys.time()
tmod_lda_2 <- textmodel_lda(dfmat_collo_clean, k = 69)
end_time <- Sys.time()
```

```{r}
head(topics(tmod_lda_2), 20)
terms(tmod_lda_2, 10)
```

```{r}
# assign topic as a new document-level variable
dfmat_collo_clean$topic <- topics(tmod_lda_2)

# cross-table of the topic frequency
table(dfmat_collo_clean$topic)
```

```{r}
# construct document-feature matrix without compound collocations
dfmat <- dfm(toks) %>% 
    # remove tokens that appeared more than 99% and less than 0.5% of the manifestos
    dfm_trim(max_termfreg = 0.99, min_termfreq = 0.05, termfreq_type = "quantile",
             # TDMs were created for each election and words that appeared in less than 0.5% of manifestos for that election were removed.
             min_docfreq = 0.005, docfreq_type = "prop")

dfmat_clean <- dfm_select(dfmat, pattern = c(stopwords_amy), selection = "remove")
print(dfmat_clean, 3)
```

```{r}
# LDA with DFM without collocation
start_time <- Sys.time()
tmod_lda_1 <- textmodel_lda(dfmat_clean, k = 69)
end_time <- Sys.time()
end_time - start_time # run a LDA model takes about 5 mins

head(topics(tmod_lda_1), 20)
terms(tmod_lda_1, 10)
```

```{r}
# assign topic as a new document-level variable
dfmat_clean$topic <- topics(tmod_lda_1)

# cross-table of the topic frequency
table(dfmat_clean$topic)
```

# Step 5-1: Conducting LDA by topicmodels package

```{r eval=FALSE, include=FALSE}
# Another LDA by topicmodels package
library(topicmodels)
dtm_1 = convert(dfmat_clean, to = "topicmodels") 
set.seed(123)
m = LDA(dtm_1, method = "Gibbs", k = 69,  control = list(alpha = 0.1))

terms(m, 10)
topic = 69
words = posterior(m)$terms[topic, ]
topwords = head(sort(words, decreasing = T), n=50)
head(topwords)

library(LDAvis)   

dtm_2 = dtm_1[slam::row_sums(dtm_1) > 0, ]
phi = as.matrix(posterior(m)$terms)
theta <- as.matrix(posterior(m)$topics)
vocab <- colnames(phi)
doc.length = slam::row_sums(dtm_2)
term.freq = slam::col_sums(dtm_2)[match(vocab, colnames(dtm_2))]

json = createJSON(phi = phi, theta = theta, vocab = vocab,
     doc.length = doc.length, term.frequency = term.freq)
serVis(json)
```

# Step 5-2: Conducting Seeded LDA coming soon...

```{r eval=FALSE, include=FALSE}
# Seeded LDA

# load dictionary containing seed words
dict_topic <- dictionary(file = "../dictionary/topics.yml") # I haven't predifine topics first
print(dict_topic)

tmod_slda <- textmodel_seededlda(dfmat_clean, dictionary = dict_topic)
terms(tmod_slda, 20)
head(topics(tmod_slda), 20)

# assign topics from seeded LDA as a document-level variable to the dfm
dfmat_clean$topic2 <- topics(tmod_slda)

# cross-table of the topic frequency
table(dfmat_clean$topic2)
```

## Step 6: Exportint the DTM as csv file and rerun it in python

```{r}
dfm_dataframe <- convert(dfmat_collo_clean, to = "data.frame")

write.csv(dfm_dataframe, "/Users/deankuo/Desktop/python/dessertation_replicate/final_collo_dfm.csv", row.names=TRUE)
```
